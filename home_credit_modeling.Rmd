---
title: "Home Credit Modeling"
author: "Bryson Burr"
date: "April 26, 2025"
output:
  html_document:
    number_sections: true
    toc: true
  pdf_document:
    toc: true
editor_options:
  chunk_output_type: console
---
# Introduction

## Business Problem
Home Credit Group aims to broaden financial inclusion for unbanked populations by providing a positive and safe borrowing experience. The key challenge is accurately predicting which loan applicants will be able to repay their loans. This prediction is critical because:

1. Rejecting good applicants means lost business opportunities
2. Approving high-risk applicants may lead to financial losses and increased default rates.
3. Many potential customers lack conventional credit histories, making traditional scoring methods inadequate

The company currently uses various statistical and machine learning methods to make these predictions but believes there's room for improvement. Our goal, and therefore the purpose of this notebook, is to develop a model that can more accurately identify which clients are capable of repayment, allowing Home Credit to make better-informed lending decisions.

## Analytical Problem
Customer repayment probability will be generated using a supervised machine-learning model. The model will use as inputs data collected on past customers such as application, demographic, and historical credit behavior. The model will use this information to predict the probability that the customers will either repay or not repay their loans. 

# Data Preperation

## Package loading

```{r setup, message = FALSE, warning = FALSE, results = 'hide'}

# Set working directory
mydir <- getwd()
setwd(mydir)

# List of required packages
required_packages <- c("tidyverse", "janitor", "psych", "skimr", "pROC", "randomForest", "gbm", "C50", "caret")

# Install any missing packages
new_packages <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)

# Load the packages
lapply(required_packages, library, character.only = TRUE)
```

## Data Import

```{r data loading, message = FALSE, warning = FALSE}

application_train = read_csv("application_train.csv", col_names = TRUE, show_col_types = FALSE)
application_test = read_csv("application_test.csv", col_names = TRUE, , show_col_types = FALSE)
```

## Convert TARGET variable to a factor

```{r factor target, message = FALSE, warning = FALSE}
# Create a working copy of the dataset
train <- application_train

# Convert TARGET variable to a factor
train$TARGET <- as.factor(train$TARGET)

# Verify the conversion
str(train$TARGET)
table(train$TARGET)  # Check class distribution
```

## Convert Categorical Variables to Factors

```{r factoring, message = FALSE, warning = FALSE}

# get rid of commas in these columns so C50 model runs
train$ORGANIZATION_TYPE <- gsub(":", "", train$ORGANIZATION_TYPE)
train$WALLSMATERIAL_MODE <- gsub(",", " and", train$WALLSMATERIAL_MODE)
train$NAME_TYPE_SUITE <- gsub(",", " and", train$NAME_TYPE_SUITE)

application_test$ORGANIZATION_TYPE <- gsub(":", "", application_test$ORGANIZATION_TYPE)
application_test$WALLSMATERIAL_MODE <- gsub(",", " and", application_test$WALLSMATERIAL_MODE)
application_test$NAME_TYPE_SUITE <- gsub(",", " and", application_test$NAME_TYPE_SUITE)

# Convert Categorical Variables to Factors

# Identify categorical variables (excluding the ID column)
cat_cols <- names(train)[sapply(train, is.character) | sapply(train, is.factor)]
cat_cols <- setdiff(cat_cols, "SK_ID_CURR") # Exclude ID column if present

# Convert to factors
for (col in cat_cols) {
  train[[col]] <- as.factor(train[[col]])
}

# Convert test to factors
cat_test_cols <- names(application_test)[sapply(application_test, is.character) | sapply(application_test, is.factor)]
cat_test_cols <- setdiff(cat_test_cols, "SK_ID_CURR")
for (col in cat_test_cols) {
  application_test[[col]] <- as.factor(application_test[[col]])
}



# Verify conversion
str(train[, cat_cols])  # Check structure of categorical columns

str(application_test[, cat_test_cols])
```

# Modeling

## Prepare Training and Validation Sets

```{r modeling prep, message = FALSE, warning = FALSE}
# Split data into training (80%) and validation (20%)
set.seed(42)  # For reproducibility

train_index <- createDataPartition(train$TARGET, p = 0.8, list = FALSE)
train_set <- train[train_index, ]
valid_set <- train[-train_index, ]

cat("Training set size:", nrow(train_set), "\n")
cat("Validation set size:", nrow(valid_set), "\n")

```

## Decision tree model

```{r decision tree, message = FALSE, warning = FALSE}
# create model, will use CF to fine tune
tree_cf_1 <- C5.0(train_set$TARGET ~. -SK_ID_CURR, train_set, control = C5.0Control(CF= .4, earlyStopping = FALSE, noGlobalPruning = FALSE))

# create predictions for training set
tree_cf_1_train_predictions <- predict(tree_cf_1,train_set, type = "prob")
train_positive_probs <- tree_cf_1_train_predictions[, "1"]

# finding auc score for training set
train_roc <- roc(train_set$TARGET, train_positive_probs)  
train_auc <- auc(train_roc) 
train_auc

# create predictions for validation set
tree_cf_1_valid_predictions <- predict(tree_cf_1,valid_set, type = "prob")
valid_positive_probs <- tree_cf_1_valid_predictions[, "1"]
valid_roc <- roc(valid_set$TARGET, valid_positive_probs)  
valid_auc <- auc(valid_roc) 
valid_auc

# adjusting CF to lower value to see if more generalized model helps
tree_cf_2 <- C5.0(train_set$TARGET ~. -SK_ID_CURR, train_set, control = C5.0Control(CF= .1, earlyStopping = FALSE, noGlobalPruning = FALSE))

# create predictions for training set
tree_cf_2_train_predictions <- predict(tree_cf_2,train_set, type = "prob")
train_positive_probs_2 <- tree_cf_2_train_predictions[, "1"]

# finding auc score for training set
train_roc_2 <- roc(train_set$TARGET, train_positive_probs_2)  
train_auc_2 <- auc(train_roc_2) 
train_auc_2

# create predictions for validation set
tree_cf_2_valid_predictions <- predict(tree_cf_2,valid_set, type = "prob")
valid_positive_probs_2 <- tree_cf_2_valid_predictions[, "1"]
valid_roc_2 <- roc(valid_set$TARGET, valid_positive_probs_2)  
valid_auc_2 <- auc(valid_roc_2) 
valid_auc_2

# dropping CF made it worse, try increasing CF
tree_cf_3 <- C5.0(train_set$TARGET ~. -SK_ID_CURR, train_set, control = C5.0Control(CF= .8, earlyStopping = FALSE, noGlobalPruning = FALSE))

# create predictions for training set
tree_cf_3_train_predictions <- predict(tree_cf_3,train_set, type = "prob")
train_positive_probs_3 <- tree_cf_3_train_predictions[, "1"]

# finding auc score for training set
train_roc_3 <- roc(train_set$TARGET, train_positive_probs_3)  
train_auc_3 <- auc(train_roc_3) 
train_auc_3

# create predictions for validation set
tree_cf_3_valid_predictions <- predict(tree_cf_3,valid_set, type = "prob")
valid_positive_probs_3 <- tree_cf_3_valid_predictions[, "1"]
valid_roc_3 <- roc(valid_set$TARGET, valid_positive_probs_3)  
valid_auc_3 <- auc(valid_roc_3) 
valid_auc_3

# Confusion Matrices
# Test 1
predictions <- as.factor(ifelse(train_positive_probs > 0.5, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = train_set$TARGET)

# Validate 1
predictions <- as.factor(ifelse(valid_positive_probs > 0.5, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = valid_set$TARGET)

# Test 2
predictions <- as.factor(ifelse(train_positive_probs_2 > 0.5, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = train_set$TARGET)

# Validate 2
predictions <- as.factor(ifelse(valid_positive_probs_2 > 0.5, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = valid_set$TARGET)

# Test 3
predictions <- as.factor(ifelse(train_positive_probs_3 > 0.5, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = train_set$TARGET)

# Validate 3
predictions <- as.factor(ifelse(valid_positive_probs_3 > 0.5, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = valid_set$TARGET)

# Validate 3 with different >
predictions <- as.factor(ifelse(valid_positive_probs_3 > 0.1, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = valid_set$TARGET)
```

We used a CF of .4 to create a baseline performance for the decision tree. It had
a training AUC of .6841, and a validation AUC of .6687. We then created another
decision tree, lowering the CF to .1 to see if a more generalized model would work
better. That model an AUC of ,5 for both the training and validation sets, meaning
this dataset needs a more specific model. In response to that, we increased the CF to .8
That model had a training AUC of .7869 and a validation AUC of .6618. This shows that
this model was too overfitted to the training data.

After converting the data into yes and no variables setting the threshold to .5 of the 
initial prediction results, we can see small changes in outcome. We did explore using a
different number than 0.5 to class objects into factor variables 0 and 1. The balanced
accuracy results improved overall as number became smaller, but at the cost of one 
side of the predictions. This means that lending would be less risky to the lender as
the algorithm would be less likely to lend to those who would default, but it also
means it would miss a significantly larger population of those individuals who
would make their payments and therefore reduce revenue. The top results using this method
shows a balanced accuracy score of 63.65% on the training set and 52.54% on the 
validation set. This means that the model is performing as expected, but isn't the
most accurate prediction method and leaves room for improvement.

We are going to try creating a decision tree with weights to try and help
with the class imbalance. More emphasis will be put on those who would trouble
repaying their loans.

### Decision tree with weights
```{r dt weights, message = FALSE, warning = FALSE}
 # Assign higher weight to 1, the minority class
weights <- ifelse(train_set$TARGET == "1", 3, 1)

#  create model with weights
tree_cf_weighted <- C5.0(train_set$TARGET ~ . -SK_ID_CURR, data = train_set, weights = weights)
# predictions for training set
tree_cf_weighted_predictions <- predict(tree_cf_weighted,train_set, type = "prob")

# calculating auc for training set
train_weighted_positive_probs <- tree_cf_weighted_predictions[, "1"]
train_weight_roc <- roc(train_set$TARGET, train_weighted_positive_probs)  
train_weight_auc <- auc(train_weight_roc)
train_weight_auc

# predictions for validation set
tree_cf_valid_predictions <- predict(tree_cf_weighted,valid_set, type = "prob")
# auc for validation set
valid_weight_probs <- tree_cf_valid_predictions[, "1"]
valid_weight_roc <- roc(valid_set$TARGET, valid_weight_probs)  
valid_weight_auc <- auc(valid_weight_roc) 
valid_weight_auc

# Confusion Matrices
# Test
predictions <- as.factor(ifelse(train_weighted_positive_probs > 0.5, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = train_set$TARGET)

# Validate
predictions <- as.factor(ifelse(valid_weight_probs > 0.5, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = valid_set$TARGET)

# Validate > 0.1
predictions <- as.factor(ifelse(valid_weight_probs > 0.1, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = valid_set$TARGET)
```

The decision tree with weights had a training AUC of .9904, with a validation AUC of .61.
This weights made the model way too overfitted to the training data. Adding in the
confusion matrix shows the overfitting more clearly as the training set is extremely
accurate, but the validation set is no more accurate than the previous models. We
see a balanced accuracy of 55.72% in the validation data, which is on slightly 
better than the previous model performance. Unlike the previous models, however,
reducing the threshold to 0.1 does not significantly improve performance. <br>

Overall, these models all took about 3 minutes to train. We will go on to the 
random forest to see if this can capture more of the complex relationships that
exist in this dataset. 

## Random Forest
```{r Random Forest, message = FALSE, warning = FALSE}
# Rf can't handle over 53 different categorical types, take out organization type
tree_rf <- randomForest(TARGET ~ . - SK_ID_CURR - ORGANIZATION_TYPE, 
                        data = train_set, 
                        ntree = 100,
                        nodesize = 10)  

tree_rf_preds <- predict(tree_rf, type = "prob")

# calculating auc for training set
train_rf_positive_probs <- tree_rf_preds[, "1"]
train_rf_roc <- roc(train_set$TARGET, train_rf_positive_probs)  
train_rf_auc <- auc(train_rf_roc)
train_rf_auc

# predictions for validation set
tree_rf_valid_predictions <- predict(tree_rf,valid_set, type = "prob")
# auc for validation set
valid_rf_probs <- tree_rf_valid_predictions[, "1"]
valid_rf_roc <- roc(valid_set$TARGET, valid_rf_probs)  
valid_rf_auc <- auc(valid_rf_roc) 
valid_rf_auc

# try increasing tree size
tree_rf_1 <- randomForest(TARGET ~ . - SK_ID_CURR - ORGANIZATION_TYPE, 
                        data = train_set, 
                        ntree = 200,
                        nodesize = 10)  

tree_rf_1_preds <- predict(tree_rf_1, type = "prob")

# calculating auc for training set
train_rf_1_probs <- tree_rf_1_preds[, "1"]
train_rf_1_roc <- roc(train_set$TARGET, train_rf_1_probs)  
train_rf_1_auc <- auc(train_rf_1_roc)
train_rf_1_auc

# predictions for validation set
tree_rf_valid_1 <- predict(tree_rf_1,valid_set, type = "prob")
# auc for validation set
valid_rf_1_probs <- tree_rf_valid_1[, "1"]
valid_rf_1_roc <- roc(valid_set$TARGET, valid_rf_1_probs)  
valid_rf_1_auc <- auc(valid_rf_1_roc) 
valid_rf_1_auc

# Confusion Matrices
# Test
predictions <- as.factor(ifelse(train_rf_1_probs > 0.5, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = train_set$TARGET)

# Validate
predictions <- as.factor(ifelse(valid_rf_1_probs > 0.5, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = valid_set$TARGET)

# Validate > 0.1
predictions <- as.factor(ifelse(valid_rf_1_probs > 0.1, 1, 0))
predictions <- factor(predictions, levels = c(0,1))
confusionMatrix(data = predictions, reference = valid_set$TARGET)
```

The first random forest model had 100 trees and a minimum size of 10 nodes. It took
about 10 minutes to train and had a training AUC of .6999 and a validation AUC of .7293.
This suggests that there may be some slight underfitting in the model. The next 
random forest had 200 trees and the same node size to see if this improved performance.
It took over 20 minutes to train, and had a training AUC of .7163 and a validation of .7349.
There was a slight increase in performance compared to the model with 100 trees, but
it did come at the cost of increased computing time. Using the confusion matrix, we can
again see the difference in total outcome. Once more we see that the lower thresholds
have a positive impact on model balance accuracy. Overall, the random forest
models performed better than the decision trees.

# Testing dataset predictions

```{r testing dataset predictions, message = FALSE, warning = FALSE}
# Use random forest
# keep sk_id_curr out of predictions
tree_rf_test_preds <- predict(tree_rf, newdata = application_test[, setdiff(names(application_test), "sk_id_curr")], type = "response")

submission_df <- data.frame(
  SK_ID_CURR = as.integer(application_test$SK_ID_CURR),  
  TARGET = tree_rf_test_preds                   
)

write.csv(submission_df, "submission.csv", row.names = FALSE)
```
